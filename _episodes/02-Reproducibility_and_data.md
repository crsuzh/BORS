---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 02-Reproducibility_and_data.md in _episodes_rmd/
source: Rmd
title: "Reproducibility and Data"
teaching: 60
exercises: 90-120
questions: 
  - "Is there a reproducibility crisis?"
  - "How do I organize projects and software code to favor reproducibility?"
  - "How do I handle data in spreadsheets to favor reproducibility?"
objectives: 
  - "Practice new habits in file and folder organization which favours reprodcibiity"
  - "Some practical tips for the use of Rstudio (optional)"
  - "Some practical tips for the use of Rstudio (optional)"
keypoints:
  - "Well organized projects are easier to reproduce"
  - "Consistency is the most important principle for coding analyses and for preparing data"
  - "Transparency increases reliability and trust and also helps my future self"
---



# 1. Is there a reproducibility crisis?

&nbsp;

>## Recall: Reproducibility vs Replicability
>
**Reproducibility** refers to the ability of a researcher to duplicate the results of a prior study using the same materials as were used by the original investigator. This requires, at minimum, the sharing of data sets, relevant metadata, analytical code, and related software.
>
**Replicability** refers to the ability of a researcher to duplicate the results of a prior study if the same procedures are followed but new data are collected.
>
S Goodman et al. [https://www.science.org/doi/10.1126/scitranslmed.aaf5027]()
{: .callout}

## Retracted Nature paper
See this example of a paper published in a prestigious outlet, the journal Nature, that had issues with replication:


![]({{ page.root }}/fig/03-01retraction.png)  
W Huang et al. [https://www.nature.com/articles/s41586-018-0311-z](). The paper contains an important finding on how a special type of RNA can affect T-cells, which could imply therapeutic effects and lead to new drugs for autoimmune diseases. 

But some of the co-authors were unable to replicate the published results and hence initiated a retraction. The retraction note states:  
“In follow-up experiments to this article, we have been **unable to replicate** key aspects of the original results.”

For more information, see the [commentary on Retraction Watch.](http://retractionwatch.com/2018/07/09/researchers-pull-nature-paper-over-first-authors-objections/)  

>## Selfish reason number 5: reproducibility helps to build your reputation  
> _“Generally, making your analyses available in this way will help you to build a reputation for being an honest and careful researcher. Should there ever be a problem with one of your papers, you will be in a very good position to defend yourself and to show that you reported everything in good faith.”_
>
F Markowetz [https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0850-7]()
{: .testimonial}




## Issues with reproducibility/replication affect many fields 
Retraction due to error or non-feasibility of replication are probably pretty rare? But attempts at replication of results becomes more frequent. For example, in some fields there have been concerted efforts of aiming to replicate larger sets of studies allowing to assess reliability of results on a wider scale.

### Biomedicine  
F Prinz et al. attempted "To substantiate our incidental observations that published reports are frequently not reproducible with quantitative data, we performed an analysis of our early (target identification and validation) in-house projects in our strategic research fields of oncology, women's health and cardiovascular diseases that were performed over the past 4 years." From 67 papers only 22 were fully or partially confirmed, see [https://www.nature.com/articles/nrd3439-c1](). 

![]({{ page.root }}/fig/03-RetractionBiomedicine.png){: height="100px"}  
### Psychology  
The Open Science Collaboration in Psychology "conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available." in an attempt to "Estimating the reproducibility of psychological science". Only 39 of 100 replication studies were significant and the mean effect size was about 50% compared to the original studies, see [https://www.science.org/doi/10.1126/science.aac4716](). 

![]({{ page.root }}/fig/03-RetractionPsychology.png){: height="100px"} 

### Economics  
C Camerer et al. replicated "18 studies published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014. All of these replications followed predefined analysis plans that were made publicly available beforehand, and they all have a statistical power of at least 90% to detect the original effect size at the 5% significance level." They found significant effects in 11 of 18 studies with a mean effect size of about  66% compared to the original studies, see [https://www.science.org/doi/10.1126/science.aaf0918]().

![]({{ page.root }}/fig/03-RetractionEconomics.png){: height="100px"}   

### Social Sciences
Again C Camerer et al. "replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 2015." They found significant effects in 13 of 21 studies with mean effect size about 50% compared to the  original studies, see [https://www.nature.com/articles/s41562-018-0399-z]().

![]({{ page.root }}/fig/03-RetractionSocialSciences.png){: height="100px"}   


## 1,500 scientists lift the lid on reproducibility
In 2016 M Baker designed a survey meant to shed "light on the ‘crisis’ rocking research." Here we discuss some of the results of the survey, for a complete report see [https://www.nature.com/articles/533452a](). The two graphs from the publication show that a large proportion of researchers beliefs that there are issues with reproducibility but that, again in the opinion of researchers, the extent of the problem differs between disciplines. Specifically, researchers from the "hard" science such as chemistry and physics, more frequently believe that the published work in their field is reproducible than for example in the "softer" sciences biology and medicine.

![]({{ page.root }}/fig/03-ReproCrisis01.png){: height="200px"} ![]({{ page.root }}/fig/03-ReproCrisis02.png){: height="150px"}  


## Factors contributing to irreproducible research  
Baker also tried to evaluate which factors could contribute to this perceived reproducibility issue. Most researchers (more thn 95%) believe that selective reporting and pressure to publish always/often or sometimes contribute to irreproducibility. Still about 90% believe that low statistical power of poor analysis, not enough replication in the original lan and insufficient mentoring/oversight always/often or sometimes contribute. Around 80% agree with unavailability of methods/code, poor experimental design, unavailability of raw data and unsufficient peer review as contributing factors at least sometimes. Fraud plays a more minor role in the opinion of researchers.

![]({{ page.root }}/fig/03-ReproCrisis04.png)  

&nbsp;

>## Selfish reason number 4: reproducibility enables continuity of your work  
>> _“I did this analysis 6 months ago. Of course I can’t remember all the details after such a long time.”_
>
F Markowetz [https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0850-7]()
{: .testimonial}



## Quiz on crisis narrative

> ## Factors contributing to irreproducibility
>
> Peaking at the content below, with which of the above factors that contributes to irreproducible research is the current episode of this course is concerned?
>
{: .challenge}

> ## Solution
> 
> Methods, code unavailable
>
{: .solution}


# 2. Organization and software

>## Selfish reason number 1: reproducibility helps to avoid disaster  
>> _“This experience showed me two things. First of all, a project is more than a beautiful result. You need to **record in detail how you got there**. And second, starting to work reproducibly early on will **save you time later**. We wasted years of our and our collaborators’ time by not being able to reproduce our own results. All of this could have been avoided by keeping better track of how the data and analyses evolved over time.”_
>
F Markowetz [https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0850-7]()
{: .testimonial}


## Project organization  
The main principles of data analytic project organization is the separation of  

- **data**  
- **method**  
- **output**  
and the preservation of the  
- **computational environment**  

>## Project organization  
> To achieve these principles make sure that you follow a procedure similar to:
1. Put each project in its own directory named after the project.
2. Put text associated documents in the `doc` directory.
3. Put raw data and metadata in a `data` directory and files generated during cleanup and analysis in a `results` directory.
4. Put project source code in the `src` directory.
5. Put external scripts or compiled programs in the `bin` directory.
6. Name all files to reflect their content or function.
>
From **Good enough practices in scientific computing** by G Wilson et al. [https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510]()  
{: .checklist}

&nbsp;

In **Packaging Data Analytical Work Reproducibly Using R (and Friends)**  B Marwick et al. [https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1375986]() suggest a slightly different but conceptually similar approach. They propose to organize projects as so-called "research compendia", for example like:

![]({{ page.root }}/fig/03-Project02.png)

![]({{ page.root }}/fig/03-Project01.png)  

 



>## Selfish reason number 3: reproducibility helps reviewers see it your way  
>> _“One of the reviewers proposed a slight change to some analyses, and because he had **access to the complete analysis**, he could directly try out his ideas on our data and see how the results changed.”_
>
F Markowetz [https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0850-7]()
{: .testimonial}

## Software/code
Writing code for a data analysis instead of using a GUI based tool makes an analysis to some degree reproducible (given the availability of the data and the analogous functioning of the computing environment). But code can also be a very detailed documentation of the employed methods, at least if it is written in a way such that it is understandable.
>## Code understandability  
Use the following principles that make code easier to understand and use by others and your future self  
>
>   1. Place a brief explanatory comment at the start of every program.  
>   2. Decompose programs into functions.  
>   3. Be ruthless about eliminating duplication.  
>   4. Search for well-maintained libraries that do what you need.  
>   5. Test libraries before relying on them.  
>   6. Give functions and variables meaningful names.  
>   7. Make dependencies and requirements explicit.  
>   8. Do not comment and uncomment code sections to control behavior.  
>   9. Provide a simple example or test data set.  
>
>
&rArr; Your main goal with these principles is for your code to be **readable, reusable, testable**  
>
From G Wilson et al. [https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510]() 
{: .checklist}

On top of these high level recommendations writing and reading code is easier if one adheres to some styling rules. Over the years I have assembled my ten most important rules for code styling in R, these were influenced by https://style.tidyverse.org, https://google.github.io/styleguide/Rguide.html,  https://cfss.uchicago.edu/notes/style-guide/ and by lot of experience in reading code by others and my past self.

>## Rules for code styling (in R)
>
1. Code styling is about readability not about correctness. The most important factor for readability is **consistency** which also increases writing efficiency. 
2. Use **white space** for readability, spaces around operators (e.g. +), after commas and before %>%, line breaks before each command and after each %>%.
3. Control the **length of your code lines** to be about 80 characters. Short statements, even loops etc, can be a single line.
4. **Indent** your code consistently, the preferred way of indentation are two spaces.
5. Use concise and informative variable names, do not use spaces, link by underscore or use CamelCase. Avoid names, that are already used, e.g., `mean`, `c`. 
6. **Comment** your code such that its structure is visible and findable (use code folding in RStudio).
7. Do not use the equal sign for assignment in R, <- is the appropriate operator for this. Avoid right-hand assignment, ->, since it deteriorates readability.
8. Curly braces are a crucial programming tool in R. The opening { should be the last character on the line, the closing } the first (and last) on the line.
9. File naming is part of good programming style. Do not use spaces or non-standard characters, use consistent and informative names.
10. Do use the assistance provided by RStudio: command/control + i and shift + command/ctrl + A.
>
{: .checklist}


>## Never change a running horse?
![]({{ page.root }}/fig/03-Meme.png)  
{: .testimonial}

https://www.reddit.com/r/ProgrammerHumor/comments/ebh05i/i_remade_this_one_think_its_funnier_this_way/



## Quiz on organization and software

> ## Directories
> Which directories would you use for cleaned data files of .csv format?
> - results  
> - data  
> - doc  
> - results/cleaneddata  
>
{: .challenge}

> ## Solution
> 
> T results  
> F data  
> F doc  
> T results/cleaneddata  
>
{: .solution}




# 3. Data in spreadsheets

>## Selfish reason number 2: reproducibility makes it easier to write papers  
>> _“Transparency in your analysis makes writing papers much easier. For example, in a dynamic document (Box 1) all **results automatically update when the data are changed**. You can be confident your numbers, figures and tables are up-to-date. Additionally, transparent analyses are more engaging, more eyes can look over them and it is much easier to spot mistakes.”_
>
F Markowetz [https://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0850-7]()
{: .testimonial}

&nbsp;

![]({{ page.root }}/fig/03-Meme02.png)  
[https://xkcd.com/2180/]()  

Data in spreadsheets is preferably organized in way that favors reproducibility. We will summarize the recommendations of the article by K Broman and K Woo [https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1375989]() into five checklists below. Broman and Woo promise that:  

“By following this advice, researchers will create spreadsheets that are **less error-prone, easier for computers to process**, and **easier to share** with collaborators and the public. Spreadsheets that adhere to our recommendations will work well with the tidy tools and reproducible methods described elsewhere in this collection and will **form the basis of a robust and reproducible analytic workflow**.”  



&nbsp;

 
>## Consistency  
>
1. Use consistent codes for categorical variables.
2. Use a consistent fixed code for any missing values.
3. Use consistent variable names.
4. Use consistent subject identifiers.
5. Use a consistent data layout in multiple files.
6. Use consistent file names.
7. Use a consistent format for all dates.
8. Use consistent phrases in your notes.
9. Be careful about extra spaces within cells.
10. Use a consistent missing value coding: empty cell/NA/~~99999~~ 
{: .checklist}

&nbsp;

>## Choose good names for files and variables
>
- No spaces  
- Use underscores or hyphens or periods (only one of them)  
- No special characters (&,*,%,ü,ä,ö,...)  
- Use a unique, short but meaningful name  
- Variable names have to start with a letter  
{: .checklist}

&nbsp;

>## Be careful with dates  
- Use the ISO 8601 global standard  
- Convention for dates in Excel is different on Windows and Mac computers  
- Dates have an internal numerical representation  
- Best to declare date columns as text, but only works prospectively  
- Consider separate year, month, day columns  
{: .checklist}

![]({{ page.root }}/fig/03-Meme03.png)
[https://xkcd.com/1179/]()


>## Make your data truly readable and rectangular 
- Put one information of the same form per cell  
- Do not add remarks in cells which should contain numerical values, e.g. >10000  
- Include one variable per column, one row per subject: a rectangle of data 
- Use the first and only the first row for variable names  
- Do not calculate means, standard deviations etc in the last row  
- Do not color, highlight or merge cells  
- Use data validation at data entry  
- Be careful with commas since they may be decimal separators  
- Consider write protecting a file at the end of data collection  
>
{: .checklist}

&nbsp;


>## Code book/data dictionary  
>
>- Create a code book in a separate sheet or file  
>- Code book contains  
>
  - a short description  
  - unit and max/min values for continuous variables  
  - all levels with their code for categorical variables  
  - ordering for ordinal variables  
>
- All variables have to be contained in the code book  
{: .checklist}

&nbsp;

Unfortunately, things can go wrong nevertheless:

![]({{ page.root }}/fig/03-Meme04.png)  
[https://starecat.com/content/wp-content/uploads/me-excel-switches-number-to-a-date-friends-joey-learning-french.jpg]()



## Quiz on data in spreadsheets

> ## Variable names
>
> What are good names for the variable containing average height per age class?
> - averageheightperageclass
> - av_height_agecls
> - height/class
> - av_height
>
{: .challenge}

> ## Solution
> 
> F averageheightperageclass  
> T av_height_agecls  
> F height/class  
> F av_height  
>
{: .solution}


> ## Ruthlessness
>
> Choose how to best initialize the variables that contain the BMI (body mass index) of 17 subjects at three different time points.
> - bmi1 <- numeric(17); bmi2 <- numeric(17); bmi3 <- numeric(17)
> - bmi <- matrix(0, nrow=17, ncol=3)
> - bmi <- NULL; ind <- c(0,0,0); for (i in 1:17) bmi <- rbind(bmi, ind)
>
{: .challenge}

> ## Solution
> 
> F bmi1 <- numeric(17); bmi2 <- numeric(17); bmi3 <- numeric(17)  
> T bmi <- matrix(0, nrow=17, ncol=3)  
> F bmi <- NULL; ind <- c(0,0,0); for (i in 1:17) bmi <- rbind(bmi, ind)  
>
{: .solution}


> ## Special care for dates
>
> This episode was created on February 28, 2023. Enter this date as an 8-digit integer:
>
{: .challenge}

> ## Solution
> 
> 20230228
>
{: .solution}


> ## Once more dates
>
> This episode was created on February 28, 2023. Enter this date in ISO 8601 coding:
>
{: .challenge}

> ## Solution
> 
> 2023-02-28
>
{: .solution}


> ## Missing values
>
> Choose all acceptable codes for missing values.
> - 99999
> - -99999
> - NA
> - 'empty cell'
> - non detectable
>
{: .challenge}

> ## Solution
> 
> F 99999  
> F -99999  
> T NA  
> T 'empty cell'  
> F non detectable  
>
{: .solution}


> ## Code styling
>
> The preferred way of indenting code is
> - a tab
> - none
> - two spaces
>
{: .challenge}

> ## Solution
> 
> F a tab  
> F none  
> T two spaces  
>
{: .solution}


# Episode challenge

>## Improve a spreadsheet in Excel
>
Considering the input on data in spreadsheets try to improve the spreadsheet
>
[trainingdata.xlsx]({{ page.root }}/files/docs/03/trainingdata.xlsx)
>
This spreadsheet contains data from 482 patients, two columns with dates and 8 columns with counts of two different markers in the blood on a baseline date, on day 1, 2 and 3 of a certain therapy.
>
>>## Specifically you should check  
>>
>> - the plausibility of all observations (e.g. value in correct range)
>> - the correct and consistent format of the entries, e.g. spelling or encoding errors 
>> - date formats 
>> - the format of missing values
>> - variable names
>> - the overall layout of the spreadsheet (header, merged cells, entries that are not observations etc.)
>{: .checklist}
{: .challenge}



>## Improve a spreadsheet in R
>
>We continue to work on the spreadsheet [trainingdata.xlsx]({{ page.root }}/files/docs/03/trainingdata.xlsx). This time we use `R` to correct the same errors in the spreadsheet. Why do you think is it better to use R for this process?
{: .challenge}

> ## Solution
> 
TBA
{: .solution}
